{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import  classification_report, accuracy_score, confusion_matrix, roc_curve, auc, precision_recall_curve, average_precision_score, fbeta_score, roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    data = pd.read_csv('../data/raw/WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
    "    churn_data = pd.DataFrame(data)\n",
    "    df = churn_data.copy()\n",
    "    df.drop(\"customerID\", axis=1, inplace=True)\n",
    "    print(\"Data set loaded.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"No data set found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_name = df.columns\n",
    "print(columns_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[np.isnan(df['TotalCharges'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"SeniorCitizen\"] = df[\"SeniorCitizen\"].map({0:\"No\", 1:\"Yes\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph of the number of people who canceled their subscription and those who did not\n",
    "# those who cancel are indicated with yes.\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x=\"Churn\", data=df)\n",
    "plt.title(\"Churn Class Distribution\")\n",
    "plt.savefig('../results/plots/Churn_class_distribution.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"Churn\"\n",
    "categorical_cols, numerical_cols = [], []\n",
    "for col in df.columns:\n",
    "    if col == target:\n",
    "        continue\n",
    "    if df[col].dtype == \"object\":\n",
    "        categorical_cols.append(col)\n",
    "    else:\n",
    "        numerical_cols.append(col)\n",
    "\n",
    "print(\"Categorical Columns:\", categorical_cols)\n",
    "print(\"Numerical Columns:\", numerical_cols)\n",
    "\n",
    "for col in categorical_cols:\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    sns.countplot(x=col, hue=target, data=df)\n",
    "    \n",
    "    # Create and sanitize title\n",
    "    title = f\"Distribution of {col} by Churn\"\n",
    "    plt.title(title)\n",
    "    \n",
    "    # Get sanitized filename from title\n",
    "    safe_title = (title.lower()\n",
    "                  .replace(\" \", \"_\")\n",
    "                  .replace(\"-\", \"\")\n",
    "                  .replace(\"/\", \"\")\n",
    "                  .replace(\"(\", \"\")\n",
    "                  .replace(\")\", \"\"))\n",
    "    \n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()  # Prevent label cutoff\n",
    "    \n",
    "    # Save with sanitized filename\n",
    "    plt.savefig(f'../results/plots/{safe_title}.png', \n",
    "                dpi=300, \n",
    "                bbox_inches='tight')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in categorical_cols:\n",
    "    col_churn = df.groupby(col)['Churn'].value_counts(normalize=True)\n",
    "    print(col_churn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "sns.scatterplot(data=df, x=\"tenure\", y=\"MonthlyCharges\", hue=target, alpha=0.6)\n",
    "plt.title(\"Tenure and MonthlyCharges with Churn\")\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/plots/Tenure_and_monthlyCharges_scatterplot.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['MonthlyChargesGroup'] = pd.cut(df['MonthlyCharges'], bins=5)\n",
    "# Churn rate pivot table\n",
    "churn_rate = df.pivot_table(values='Churn', \n",
    "                            index='PaymentMethod', \n",
    "                            columns='MonthlyChargesGroup', \n",
    "                            observed=False,\n",
    "                            aggfunc=lambda x: (x == 'Yes').mean())\n",
    "\n",
    "# Costumer number pivot  table\n",
    "customer_count = df.pivot_table(values='Churn', \n",
    "                                index='PaymentMethod', \n",
    "                                columns='MonthlyChargesGroup', \n",
    "                                observed=False,\n",
    "                                aggfunc='count')\n",
    "\n",
    "# Rate and number of customer are combined\n",
    "churn_rate_rounded = churn_rate.round(2) \n",
    "combined_data = churn_rate_rounded.astype(str) + \"\\n(\" + customer_count.astype(int).astype(str) + \")\"\n",
    "\n",
    "# Heatmap table\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(churn_rate, annot=combined_data,  fmt=\"\", cmap='coolwarm', cbar_kws={'label': 'Churn Rate'})\n",
    "plt.title('Average Churn Rate and Number of Customers with MonthlyCharges and PaymentMethod')\n",
    "plt.xlabel('MonthlyCharges Group')\n",
    "plt.ylabel('PaymentMethod')\n",
    "plt.savefig('../results/plots/Churn_rate_heatmap.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['MonthlyChargesGroup'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in numerical_cols:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.boxplot(x=target, y=col, data=df)\n",
    "    title = f\"Distribution of {col} by Churn\"  \n",
    "    plt.title(title)\n",
    "    safe_title = (title.lower()\n",
    "                  .replace(\" \", \"_\")\n",
    "                  .replace(\"-\", \"\")\n",
    "                  .replace(\"/\", \"\")\n",
    "                  .replace(\"(\", \"\")\n",
    "                  .replace(\")\", \"\"))\n",
    "    \n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()  # Prevent label cutoff\n",
    "    \n",
    "    # Save with sanitized filename\n",
    "    plt.savefig(f'../results/plots/{safe_title}.png', \n",
    "                dpi=300, \n",
    "                bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in numerical_cols:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.histplot(df[col], kde=True, bins=30)\n",
    "    plt.title(col)\n",
    "    plt.show()\n",
    "    skewness = df[col].skew()\n",
    "    print(f\"(Skewness): {skewness}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Churn'] = df['Churn'].map({'Yes': 1, 'No': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Assuming df, numerical_cols, and categorical_cols are defined\n",
    "\n",
    "X = df.drop(columns=['Churn'])\n",
    "y = df['Churn']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "class HighRiskFeatureGenerator(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.train_80th_ = None\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.train_80th_ = X['MonthlyCharges'].quantile(0.8)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X['HighRiskCustomers'] = (\n",
    "            (X['PaymentMethod'] == 'Electronic check') & \n",
    "            (X['MonthlyCharges'] > self.train_80th_)\n",
    "        ).astype(int)\n",
    "        return X\n",
    "\n",
    "class DataFrameImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, numerical_cols, strategy='median'):\n",
    "        self.numerical_cols = numerical_cols\n",
    "        self.strategy = strategy\n",
    "        self.imputer = SimpleImputer(strategy=strategy)\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.imputer.fit(X[self.numerical_cols])\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X[self.numerical_cols] = self.imputer.transform(X[self.numerical_cols])\n",
    "        return X\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', DataFrameImputer(numerical_cols, strategy='median')),\n",
    "    ('feature_engineer', HighRiskFeatureGenerator()),\n",
    "    ('preprocessor', ColumnTransformer([\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "    ], remainder='passthrough')),\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('classifier', XGBClassifier())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train model\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "y_pred_xgb = pipeline.predict(X_test)\n",
    "y_prob_xgb = pipeline.predict_proba(X_test)[:, 1]  # Probabilities for positive class\n",
    "\n",
    "report = classification_report(y_test, y_pred_xgb)\n",
    "auc_score = roc_auc_score(y_test, y_prob_xgb)\n",
    "\n",
    "# Print classification metrics\n",
    "print(f\"\\nClassification Report (AUC-ROC = {auc_score:.3f}):\\n{report}\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_xgb):.3f}\")\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob_xgb)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_xgb)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob_xgb)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'AUC = {auc_score:.2f}')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall Curve\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_prob_xgb)\n",
    "avg_precision = average_precision_score(y_test, y_prob_xgb)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(recall, precision, color='blue', lw=2, \n",
    "         label=f'Precision-Recall Curve (AP = {avg_precision:.2f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "joblib.dump({\n",
    "    'pipeline': pipeline,\n",
    "    'feature_names': X_train.columns.tolist(),\n",
    "    'metrics': {'auc': auc_score, 'accuracy': accuracy_score(y_test, y_pred_xgb)}\n",
    "}, 'churn_model_metadata.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "new_pipeline = clone(pipeline)\n",
    "\n",
    "scale_pos_weight_resampled = 1.0\n",
    "\n",
    "cv = StratifiedShuffleSplit(\n",
    "    n_splits=1,           \n",
    "    test_size=0.2,        \n",
    "    random_state=42       \n",
    ")\n",
    "\n",
    "# Define grid\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [100, 200],\n",
    "    'classifier__max_depth': [3, 5],\n",
    "    'classifier__learning_rate': [0.05, 0.1],\n",
    "    'classifier__subsample': [0.8, 1.0],\n",
    "    'classifier__colsample_bytree': [0.8, 1.0],\n",
    "    'classifier__gamma': [0, 0.2],\n",
    "    'classifier__reg_alpha': [0, 0.5],\n",
    "    'classifier__scale_pos_weight': [1]  # SMOTE balances classes\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=new_pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring='roc_auc',\n",
    "    cv=cv,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Execute grid search\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_grid_model = grid_search.best_estimator_\n",
    "\n",
    "classifier = best_grid_model.named_steps['classifier']\n",
    "preprocessor = best_grid_model.named_steps['preprocessor']\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "\n",
    "y_pred_grid = best_grid_model.predict(X_test)\n",
    "y_prob_grid = best_grid_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "baseline = y_test.mean()\n",
    "\n",
    "# Include both models for comparison\n",
    "for model, color, label in zip([pipeline, best_grid_model],  # Add original pipeline\n",
    "                             ['blue', 'red'],                # Two colors\n",
    "                             ['Original', 'Grid Tuned']):    # Two labels\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        y_prob = model.decision_function(X_test)\n",
    "        \n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
    "    average_precision = average_precision_score(y_test, y_prob)\n",
    "    \n",
    "    plt.plot(recall, precision, color=color, lw=2,\n",
    "             label=f'{label} (AP = {average_precision:.2f})')\n",
    "\n",
    "plt.hlines(y=baseline, xmin=0, xmax=1, \n",
    "           colors='k', linestyles='--', \n",
    "           label=f'Baseline (AP = {baseline:.2f})')\n",
    "\n",
    "plt.title('Precision-Recall Curve Comparison')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.xlim([0.0, 1.05])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_test, y_pred_grid)\n",
    "auc_score = roc_auc_score(y_test, y_prob_grid)\n",
    "# Print classification metrics\n",
    "print(f\"\\nClassification Report (AUC-ROC = {auc_score:.3f}):\\n{report}\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_grid):.3f}\")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "for model, color, label in zip([pipeline, best_grid_model], \n",
    "                              ['blue', 'red'], \n",
    "                              ['Original', 'Grid Tuned']):\n",
    "    fpr, tpr, _ = roc_curve(y_test, model.predict_proba(X_test)[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, color=color, lw=2, label=f'{label} (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.title('ROC Curve Comparison')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate Confusion Matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_grid), \n",
    "            annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Grid Tuned Confusion Matrix')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "importances = classifier.feature_importances_\n",
    "sorted_idx = importances.argsort()\n",
    "\n",
    "# Plot top 15 features\n",
    "top_n = 15\n",
    "plt.barh(\n",
    "    feature_names[sorted_idx][-top_n:],\n",
    "    importances[sorted_idx][-top_n:],\n",
    "    color='skyblue'\n",
    ")\n",
    "plt.title(f'Top {top_n} Feature Importance')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "joblib.dump(best_grid_model, 'xgboost_grid_tuned.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
